{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7db03cb",
      "metadata": {
        "id": "e7db03cb"
      },
      "source": [
        "# Sieci rekurencyjne\n",
        "\n",
        "## Zadanie: Sieci rekurencyjne dla szeregów czasowych\n",
        "Zbiór `jena_climate_2009_2016.csv` zawiera dane klimatyczne próbkowane co 10 minut.\n",
        "1. Przygotuj dane do modelowania dzieląc zbiór na sekwencje składające się ze 120 pomiarów odpowiadających 5 dniom. Jako wartość docelową (`target`) ustaw temperaturę 24 godziny po końcu każdej sekwencji. Zastosuj metodę `timeseries_dataset_from_array()`. Skaluj dane i podziel je na zbiór treningowy, walidacyjny i testowy.\n",
        "2. Stwórz i wytrenuj sieć składającą się z warstw LSTM. Na wyjściu zastosuj warstwę `Dense(1)` i `mse` jako funkcję błędu.\n",
        "3. Zastosuj zbiór walidacyjny do porównania sieci o różnych architekturach. Testuj sieci z 1-2 warstwami LSTM o różnych wymiarach. Warstwy można regularyzować ustawiając parametr `recurrent_dropout`. Narysuj krzywe uczenia na danych treningowych i walidacyjnych. Następnie wykorzystaj zbiór testowy do ewaluacji wybranej sieci."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47af872e",
      "metadata": {
        "id": "47af872e",
        "outputId": "5ef405c7-6c9c-42cd-fc98-354553df0422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'jena_climate_2009_2016.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bd2b412c5e97>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jena_climate_2009_2016.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jena_climate_2009_2016.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "df = pd.read_csv(\"jena_climate_2009_2016.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45035110",
      "metadata": {
        "id": "45035110"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=\"Date Time\").to_numpy()\n",
        "y = df[\"T (degC)\"].to_numpy()\n",
        "\n",
        "N = X.shape[0]\n",
        "train_size = int(0.5 * N)\n",
        "val_size = int(0.25 * N)\n",
        "test_size = N - train_size - val_size\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X[:train_size])\n",
        "X = scaler.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92748f28",
      "metadata": {
        "id": "92748f28"
      },
      "outputs": [],
      "source": [
        "sampling_rate = 6 # resample hourly\n",
        "sequence_length = 120 # input sequences span 120 hours = 5 days\n",
        "delay = sampling_rate * (sequence_length + 24 - 1) # targets delayed by 24h\n",
        "batch_size = 256\n",
        "\n",
        "train_data = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    X[:-delay],\n",
        "    targets=y[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=train_size\n",
        ")\n",
        "\n",
        "val_data = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    X[:-delay],\n",
        "    targets=y[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=train_size,\n",
        "    end_index=(train_size + val_size)\n",
        ")\n",
        "\n",
        "test_data = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    X[:-delay],\n",
        "    targets=y[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=train_size + val_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1746d494",
      "metadata": {
        "id": "1746d494"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(sequence_length, X.shape[-1])),\n",
        "    tf.keras.layers.LSTM(16),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"mse\",\n",
        "              metrics=[\"mae\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5869d4a3",
      "metadata": {
        "id": "5869d4a3",
        "outputId": "ededff89-cfc9-4efa-9dfa-19fc1fdfe295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 61ms/step - loss: 81.3392 - mae: 6.8777 - val_loss: 17.4355 - val_mae: 3.1250\n",
            "Epoch 2/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 76ms/step - loss: 15.9767 - mae: 3.0263 - val_loss: 11.5170 - val_mae: 2.6012\n",
            "Epoch 3/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 111ms/step - loss: 11.4291 - mae: 2.6160 - val_loss: 10.1496 - val_mae: 2.4811\n",
            "Epoch 4/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 120ms/step - loss: 10.4344 - mae: 2.5197 - val_loss: 9.3087 - val_mae: 2.3709\n",
            "Epoch 5/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 86ms/step - loss: 9.7019 - mae: 2.4271 - val_loss: 9.3669 - val_mae: 2.3742\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x17ff843d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_data,\n",
        "          validation_data=val_data,\n",
        "          epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e2476a",
      "metadata": {
        "id": "61e2476a"
      },
      "source": [
        "## Zadanie: Przetwarzanie tekstu sieciami rekurencyjnymi\n",
        "\n",
        "1. Zbiór `aclImdb` zawiera przykłady pozytywnych (`pos`) i negatywnych (`neg`) recenzji filmów. Wczytaj zbiór za pomocą metody `text_dataset_from_directory()`. Dane są podzielone na zbiór treningowy i testowy - należy wczytać je osobno.\n",
        "2. Stwórz warstwę `TextVectorization` z parametrem `output_modes='int'` i zastosuj metodę `adapt()` aby wygenerować słownik na podstawie zbioru treningowego.\n",
        "3. Stwórz sieć LSTM i zastosuj ją do klasyfikacji recenzji. Bezpośrednio po warstwie wejściowej dodaj stworzoną wcześniej warstwę `TextVectorization`. Bezpośrednio po niej dodaj warstwę wektoryzującą słowa z tekstów. Może być to na przykład warstwa `Embedding`. Na wyjściu sieci dodaj warstwę `Dense(1, activation='sigmoid')`.\n",
        "4. Porównaj działanie sieci rekurencyjnych o różnych architekturach. Zamiast warstw LSTM można stosować też warstwy GRU. Do regularyzaji warstw można stosować `recurrent_dropout`. Narysuj krzywe uczenia na danych treningowych i walidacyjnych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3497aa9",
      "metadata": {
        "id": "d3497aa9",
        "outputId": "7ce9f481-fcbf-4d0c-d85a-7da5df292760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837c83dc",
      "metadata": {
        "id": "837c83dc",
        "outputId": "8d6c1e74-0ba2-4170-8fba-9250978df0f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-30 23:27:56.623348: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "max_tokens = 5000\n",
        "max_len = 600\n",
        "\n",
        "text_vectorization = tf.keras.layers.TextVectorization(output_mode=\"int\", max_tokens=max_tokens, output_sequence_length=max_len)\n",
        "text_vectorization.adapt(train_data.map(lambda x, y: x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314b60ac",
      "metadata": {
        "id": "314b60ac",
        "outputId": "6442b3e7-68d9-4ff4-a05b-9d5e014e843d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 309ms/step - accuracy: 0.7193 - loss: 0.5374 - val_accuracy: 0.8526 - val_loss: 0.3634\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 195ms/step - accuracy: 0.8700 - loss: 0.3155 - val_accuracy: 0.8744 - val_loss: 0.2987\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 180ms/step - accuracy: 0.9067 - loss: 0.2407 - val_accuracy: 0.8512 - val_loss: 0.3330\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 193ms/step - accuracy: 0.9230 - loss: 0.1999 - val_accuracy: 0.8226 - val_loss: 0.5118\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 187ms/step - accuracy: 0.8777 - loss: 0.3063 - val_accuracy: 0.8635 - val_loss: 0.3588\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x31ba811d0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1,), dtype=\"string\"),\n",
        "    text_vectorization,\n",
        "    tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=128, mask_zero=True),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(train_data, validation_data=valid_data, epochs=5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}